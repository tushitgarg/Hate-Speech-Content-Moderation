{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ImplementingFasttext.ipynb","provenance":[],"authorship_tag":"ABX9TyNt50lZrUspJmEjTsrkliWh"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"NhGZbFz7JcSk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":489},"outputId":"577face0-7694-47ac-9e7a-7a384b5ee422","executionInfo":{"status":"ok","timestamp":1583257606459,"user_tz":-330,"elapsed":33026,"user":{"displayName":"Tushit Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTSRCYgv67Ofc631qm0oqaSeAUqs23f9GgD-2XqQ=s64","userId":"03813354428158958398"}}},"source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import json\n","import pymongo\n","from pymongo import MongoClient\n","import pprint as pp\n","! pip install normalise\n","import nltk\n","nltk.download('names')\n","nltk.download('brown')\n","\n","import numpy as np\n","import multiprocessing as mp\n","\n","import string\n","import spacy \n","import en_core_web_sm\n","from nltk.tokenize import word_tokenize\n","from sklearn.base import TransformerMixin, BaseEstimator\n","from normalise import normalise\n","import re\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GRU, SimpleRNN\n","from keras.layers.embeddings import Embedding\n","from keras.initializers import Constant\n","from keras import layers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting normalise\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/2d/f06cf3d3714502dec10e19238a5da201b71ce198165beda9c1adaf5063da/normalise-0.1.8-py3-none-any.whl (15.7MB)\n","\u001b[K     |████████████████████████████████| 15.7MB 301kB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from normalise) (0.22.1)\n","Collecting roman\n","  Downloading https://files.pythonhosted.org/packages/8d/f2/29d1d069555855ed49c74b627e6af73cec7a5f4de27c200ea0d760939da4/roman-3.2-py2.py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.17.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from normalise) (3.2.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->normalise) (0.14.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->normalise) (1.12.0)\n","Installing collected packages: roman, normalise\n","Successfully installed normalise-0.1.8 roman-3.2\n","[nltk_data] Downloading package names to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/names.zip.\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.semi_supervised.label_propagation module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.semi_supervised. Anything that cannot be imported from sklearn.semi_supervised is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n","  UserWarning)\n","Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"YPmgBxLXJnGM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"outputId":"7b721823-0eee-4e4b-f7a5-7828ef49187a","executionInfo":{"status":"ok","timestamp":1583257656380,"user_tz":-330,"elapsed":82920,"user":{"displayName":"Tushit Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTSRCYgv67Ofc631qm0oqaSeAUqs23f9GgD-2XqQ=s64","userId":"03813354428158958398"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KcY-KtWHR_7m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"e86ccb6d-efa4-4e01-bfe1-433d94086e5d","executionInfo":{"status":"ok","timestamp":1583305833350,"user_tz":-330,"elapsed":2184,"user":{"displayName":"Tushit Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTSRCYgv67Ofc631qm0oqaSeAUqs23f9GgD-2XqQ=s64","userId":"03813354428158958398"}}},"source":["cd '/content/drive/My Drive/minor2/twitter2/'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/My Drive/minor2/twitter2/'\n","/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jji861mcJrk8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"43abfcda-6f21-442e-dcc5-4349b39f1703","executionInfo":{"status":"error","timestamp":1583305833355,"user_tz":-330,"elapsed":1850,"user":{"displayName":"Tushit Garg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTSRCYgv67Ofc631qm0oqaSeAUqs23f9GgD-2XqQ=s64","userId":"03813354428158958398"}}},"source":["df1=pd.read_excel('twitter2_data.xlsx')\n","df1.drop(['level_0','index','Unnamed: 0'],axis=1,inplace=True)\n","df1.head()"],"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-187083c5944b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twitter2_data.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'level_0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"code","metadata":{"id":"fS6TsQNPJv7C","colab_type":"code","colab":{}},"source":["np.mean(df1['label'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"izKIRHdZJ5Ey","colab_type":"code","colab":{}},"source":["text=df1['text']\n","labels=df1['label']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_aWAxnwCJ8Oz","colab_type":"code","colab":{}},"source":["nlp = en_core_web_sm.load()\n","\n","\n","class TextPreprocessor(BaseEstimator, TransformerMixin):\n","    def __init__(self,\n","                 variety=\"BrE\",\n","                 user_abbrevs={},\n","                 n_jobs=1):\n","        \"\"\"\n","        Text preprocessing transformer includes steps:\n","            1. Text normalization\n","            2. Punctuation removal\n","            3. Stop words removal\n","            4. Lemmatization\n","        \n","        variety - format of date (AmE - american type, BrE - british format) \n","        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n","        n_jobs - parallel jobs to run\n","        \"\"\"\n","        self.variety = variety\n","        self.user_abbrevs = user_abbrevs\n","        self.n_jobs = n_jobs\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X, *_):\n","        X_copy = X.copy()\n","        return X_copy.apply(self._preprocess_text)\n","\n","    def _preprocess_part(self, part):\n","        return part.apply(self._preprocess_text)\n","\n","    def _preprocess_text(self, text):\n","        text=self._clean_text(text)\n","        normalized_text = self._normalize(text)\n","        doc = nlp(normalized_text)\n","        removed_punct = self._remove_punct(doc)\n","        removed_stop_words = self._remove_stop_words(removed_punct)\n","        return self._lemmatize(removed_stop_words)\n","\n","    def _normalize(self, text):\n","        # some issues in normalise package\n","        try:\n","            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n","        except:\n","            return text\n","\n","    def _remove_punct(self, doc):\n","        return [t for t in doc if t.text not in string.punctuation]\n","\n","    def _remove_stop_words(self, doc):\n","        return [t for t in doc if not t.is_stop]\n","\n","    def _lemmatize(self, doc):\n","        return ' '.join([t.lemma_ for t in doc])\n","    \n","    \n","    def _clean_text(self,text):\n","      replace_1 = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n","      replace_2 = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n","      text=re.sub(r\"http\\S+\", \"\", text)\n","      text = replace_1.sub(\"\", text)\n","      text = replace_2.sub(\" \", text)\n","      text=re.sub('\\s+',' ',text)\n","      return text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8d4j3TNKcpd","colab_type":"code","colab":{}},"source":["%%time\n","text = TextPreprocessor(n_jobs=-1).transform(df1['text'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwHDjKKrKgNh","colab_type":"code","colab":{}},"source":["exdf=pd.DataFrame()\n","exdf['text']=[\"hello my \\n is ./.,is tushit's \\t @344$%% what are you D.R   running  doing?? please HELP!!\"]\n","clean_ex = TextPreprocessor(n_jobs=-1).transform(exdf['text'])\n","print(clean_ex[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E_SfDh9PNFJ7","colab_type":"code","colab":{}},"source":["text=text.tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YlKj-4RrNG5Z","colab_type":"code","colab":{}},"source":["text2=[]\n","for i in text:\n","    lst=i.split()\n","    text2.append(lst)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aGr48A7ONQPb","colab_type":"code","colab":{}},"source":["from gensim.models import FastText\n","model_fasttext = FastText(text2, size=100, window=5, min_count=5, workers=4,sg=0)\n","words = list(model_fasttext.wv.vocab)\n","print('Vocabulary size:', len(words))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DMT_CTI9NTsV","colab_type":"code","colab":{}},"source":["model_fasttext.wv.save_word2vec_format('/content/drive/My Drive/minor2/twitter2/trained_fasttext.txt',binary=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LRag-go-NVQ6","colab_type":"code","colab":{}},"source":["embeddings_index={}\n","f= open('/content/drive/My Drive/minor2/twitter2/trained_fasttext.txt',encoding='utf-8')\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.array(values[1:])\n","  embeddings_index[word]=coefs\n","f.close"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NAZ8YkuNW_d","colab_type":"code","colab":{}},"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","tokenizer_obj = Tokenizer()\n","tokenizer_obj.fit_on_texts(text2)\n","sequences = tokenizer_obj.texts_to_sequences(text2)\n","max_length = max([len(s.split()) for s in text])\n","word_index = tokenizer_obj.word_index\n","print('Found %s unique tokens.',len(word_index))\n","\n","tweets_pad = pad_sequences(sequences,maxlen=max_length)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xUcjvo8NhUn","colab_type":"code","colab":{}},"source":["df3=pd.concat([pd.DataFrame(text),pd.DataFrame(tweets_pad)],axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZnczHXPKNjCS","colab_type":"code","colab":{}},"source":["num_words = len(word_index)+1\n","embedding_matrix = np.zeros((num_words,100))\n","\n","for word , i in word_index.items():\n","  if i > num_words:\n","    continue\n","  embedding_vector = embeddings_index.get(word)\n","  if embedding_vector is not None:\n","    embedding_matrix[i]=embedding_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XaUfl3UWyXf","colab_type":"code","colab":{}},"source":["embedding_matrix.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2MfQVvstW1bd","colab_type":"code","colab":{}},"source":["print(num_words)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S9vofHqONw9u","colab_type":"code","colab":{}},"source":["model = Sequential()\n","embedding_layer = Embedding(num_words,\n","                            100,\n","                            embeddings_initializer = Constant(embedding_matrix),\n","                            weights = [embedding_matrix],\n","                            input_length = max_length,\n","                            trainable=False)\n","model.add(embedding_layer)\n","model.add(layers.Conv1D(128, 5, activation='relu'))\n","model.add(layers.GlobalMaxPooling1D())\n","model.add(layers.Dense(10, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzJLywnIOLhQ","colab_type":"code","colab":{}},"source":["model = Sequential()\n","embedding_layer = Embedding(num_words,\n","                            100,\n","                            embeddings_initializer = Constant(embedding_matrix),\n","                            input_length = max_length,\n","                            trainable=False)\n","model.add(embedding_layer)\n","model.add(LSTM(units=100))\n","model.add(Dropout(.3))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bwNgJvH1OOwD","colab_type":"code","colab":{}},"source":["model = Sequential()\n","embedding_layer = Embedding(num_words,\n","                            100,\n","                            embeddings_initializer = Constant(embedding_matrix),\n","                            input_length = max_length,\n","                            trainable=False)\n","model.add(embedding_layer)\n","model.add(GRU(units=100,dropout=0.2 , recurrent_dropout=0.2))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tb_5SymgCFWb","colab_type":"code","colab":{}},"source":["model = Sequential()\n","embedding_layer = Embedding(num_words,\n","                            100,\n","                            embeddings_initializer = Constant(embedding_matrix),\n","                            input_length = max_length,\n","                            trainable=False)\n","model.add(embedding_layer)\n","model.add(Flatten())\n","model.add(layers.Dense(10, activation='relu'))\n","#model.add(GRU(units=100,dropout=0.2 , recurrent_dropout=0.2))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EC1oXYkzzenE","colab_type":"code","colab":{}},"source":["model = Sequential()\n","embedding_layer = Embedding(num_words,\n","                            100,\n","                            embeddings_initializer = Constant(embedding_matrix),\n","                            input_length = max_length,\n","                            trainable=False)\n","model.add(embedding_layer)\n","#model.add(Flatten())\n","model.add(SimpleRNN(32, dropout=.2, recurrent_dropout=.2))\n","model.add(layers.Dense(10, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQ6JfuZrNzGq","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_testing ,y_train, y_testing = train_test_split(df3, labels, test_size = 0.60, random_state = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmkCtrTQN1VN","colab_type":"code","colab":{}},"source":["X_test, X_val ,y_test, y_val = train_test_split(X_testing, y_testing, test_size = 0.50, random_state = 0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ypFrWI-N3P0","colab_type":"code","colab":{}},"source":["print('Training.....')\n","history=model.fit(X_train.iloc[:,1:], y_train, batch_size=32,epochs=10,validation_data=(X_val.iloc[:,1:],y_val),verbose=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RXmcZI2yN5LJ","colab_type":"code","colab":{}},"source":["loss, accuracy = model.evaluate(X_test.iloc[:,1:], y_test, verbose=True)\n","print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EYg0B1I7ODbq","colab_type":"code","colab":{}},"source":["y_classes = model.predict_classes(X_test.iloc[:,1:])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhrk2vjmXwTm","colab_type":"code","colab":{}},"source":["dataset = pd.DataFrame({'y_test': y_test, 'y_classes': y_classes[:,0]})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGEcjkKBXyt6","colab_type":"code","colab":{}},"source":["df_x=dataset[dataset['y_test']==dataset['y_classes']]\n","df_x[df_x['y_classes']==0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5j2PpZaX31o","colab_type":"code","colab":{}},"source":["dataset[dataset['y_test']==1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Q_o14MUX6uR","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DnV7Y9g-OF7F","colab_type":"code","colab":{}},"source":["# accuracy: (tp + tn) / (p + n)\n","accuracy = accuracy_score(y_test, y_classes)\n","print('Accuracy:{}\\n'.format(accuracy))\n","# precision tp / (tp + fp)\n","precision = precision_score(y_test, y_classes)\n","print('Precision:{}\\n'.format(precision))\n","# recall: tp / (tp + fn)\n","recall = recall_score(y_test, y_classes)\n","print('Recall:{}\\n'.format(recall))\n","# f1: 2 tp / (2 tp + fp + fn)\n","f1 = f1_score(y_test, y_classes)\n","print('F1 score:{}\\n'.format(f1))\n","matrix = confusion_matrix(y_test, y_classes)\n","print(\"Confusion Matrix:\")\n","print(matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9NYwU_w_N60f","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","\n","def plot_history(history,name_of_fig):\n","    acc = history.history['acc']\n","    val_acc = history.history['val_acc']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    x = range(1, len(acc) + 1)\n","\n","    f=plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(x, acc, 'b', label='Training acc')\n","    plt.plot(x, val_acc, 'r', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(x, loss, 'b', label='Training loss')\n","    plt.plot(x, val_loss, 'r', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()\n","    f.savefig(name_of_fig, bbox_inches='tight', dpi=400)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AioE8j0lN9BI","colab_type":"code","colab":{}},"source":["plot_history(history,'/content/drive/My Drive/minor2/twitter2/accuracy and losses graphs(fasttext_LSTM).jpg')"],"execution_count":0,"outputs":[]}]}