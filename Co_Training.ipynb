{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Co Training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UjLUIozFNo99",
        "twRDck_4Nu7_",
        "krTAfaj8NJ2J",
        "qWMl6ujeNQP8",
        "SYi4pRdYPJ-F",
        "9nGNDI5hNYBz"
      ],
      "mount_file_id": "1HKNpo5eSHoNZpz4XY99v38qFX0UyF-qy",
      "authorship_tag": "ABX9TyOnN+fxzVpeFsQJ8LgCgSiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tushitgarg/Hate-Speech-Content-Moderation/blob/master/Co_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjLUIozFNo99",
        "colab_type": "text"
      },
      "source": [
        "# Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEXHHI2D-qwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "db6c6641-e4f4-44e3-e573-05df51c634a8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "import pprint as pp\n",
        "! pip install normalise\n",
        "import nltk\n",
        "nltk.download('names')\n",
        "nltk.download('brown')\n",
        "\n",
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "\n",
        "import string\n",
        "import spacy \n",
        "import en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from normalise import normalise\n",
        "import re\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GRU, SimpleRNN\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras import layers\n",
        "from keras.initializers import Constant"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting normalise\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/2d/f06cf3d3714502dec10e19238a5da201b71ce198165beda9c1adaf5063da/normalise-0.1.8-py3-none-any.whl (15.7MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7MB 272kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.18.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from normalise) (0.22.2.post1)\n",
            "Collecting roman\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/f2/29d1d069555855ed49c74b627e6af73cec7a5f4de27c200ea0d760939da4/roman-3.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from normalise) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->normalise) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->normalise) (1.12.0)\n",
            "Installing collected packages: roman, normalise\n",
            "Successfully installed normalise-0.1.8 roman-3.2\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.semi_supervised.label_propagation module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.semi_supervised. Anything that cannot be imported from sklearn.semi_supervised is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twRDck_4Nu7_",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILunTKA5JcZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,\n",
        "                 variety=\"BrE\",\n",
        "                 user_abbrevs={},\n",
        "                 n_jobs=1):\n",
        "        \"\"\"\n",
        "        Text preprocessing transformer includes steps:\n",
        "            1. Text normalization\n",
        "            2. Punctuation removal\n",
        "            3. Stop words removal\n",
        "            4. Lemmatization\n",
        "        \n",
        "        variety - format of date (AmE - american type, BrE - british format) \n",
        "        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n",
        "        n_jobs - parallel jobs to run\n",
        "        \"\"\"\n",
        "        self.variety = variety\n",
        "        self.user_abbrevs = user_abbrevs\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, *_):\n",
        "        X_copy = X.copy()\n",
        "        return X_copy.apply(self._preprocess_text)\n",
        "\n",
        "    def _preprocess_part(self, part):\n",
        "        return part.apply(self._preprocess_text)\n",
        "\n",
        "    def _preprocess_text(self, text):\n",
        "        text=self._clean_text(text)\n",
        "        normalized_text = self._normalize(text)\n",
        "        doc = nlp(normalized_text)\n",
        "        removed_punct = self._remove_punct(doc)\n",
        "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
        "        return self._lemmatize(removed_stop_words)\n",
        "\n",
        "    def _normalize(self, text):\n",
        "        # some issues in normalise package\n",
        "        try:\n",
        "            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def _remove_punct(self, doc):\n",
        "        return [t for t in doc if t.text not in string.punctuation]\n",
        "\n",
        "    def _remove_stop_words(self, doc):\n",
        "        return [t for t in doc if not t.is_stop]\n",
        "\n",
        "    def _lemmatize(self, doc):\n",
        "        return ' '.join([t.lemma_ for t in doc])\n",
        "    \n",
        "    \n",
        "    def _clean_text(self,text):\n",
        "      replace_1 = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
        "      replace_2 = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "      try:\n",
        "        text=re.sub(r\"http\\S+\", \"\", text)\n",
        "      except:\n",
        "        print(text)\n",
        "      text = replace_1.sub(\"\", text)\n",
        "      text = replace_2.sub(\" \", text)\n",
        "      text=re.sub('\\s+',' ',text)\n",
        "      return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZTA2ev__aWk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f034dbca-84d3-4e8d-c574-627e9d60df43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCYubLAJ_m-Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82c1c6ea-821a-4f57-c0eb-2a4a543afec4"
      },
      "source": [
        "cd '/content/drive/My Drive/minor2/Hate-Speech-Content-Moderation/'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/minor2/Hate-Speech-Content-Moderation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EomhQ_QnM_EE",
        "colab_type": "text"
      },
      "source": [
        "# Extracting text and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbStT9jZK9kV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_text=twitter['text']\n",
        "twitter_labels=twitter['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLHRCUPuK9pA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora_text=quora['text']\n",
        "quora_labels = quora['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dkkch82K9vW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_text=wiki['text']\n",
        "wiki_labels=wiki['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWMl6ujeNQP8",
        "colab_type": "text"
      },
      "source": [
        "# Splitting the text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSY5-8CVJxf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_text2=[]\n",
        "for i in twitter_text:\n",
        "      lst=i.split()\n",
        "      twitter_text2.append(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KrsdVX2KFqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora_text2=[]\n",
        "for i in quora_text:\n",
        "      lst=i.split()\n",
        "      quora_text2.append(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRZRH1nHKF-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_text2=[]\n",
        "for i in wiki_text:\n",
        "      lst=i.split()\n",
        "      wiki_text2.append(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nGNDI5hNYBz",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizing and padding the texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt5eieQxKQlW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f3df9cf1-14ac-45d0-dc54-553de6623142"
      },
      "source": [
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(twitter_text2) \n",
        "tus = [\"excited\"]\n",
        "sequences = tokenizer.texts_to_sequences(tus)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.',len(word_index))\n",
        "#twitter_maxlen = max([len(s.split()) for s in ' '.join(tus)])\n",
        "tweets_pad = pad_sequences(sequences, maxlen=1)\n",
        "print(tweets_pad.shape) \n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found %s unique tokens. 14884\n",
            "(1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZfEZoKJiu8Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "86431574-9e6f-4227-c4e9-01a5bb2e48a1"
      },
      "source": [
        "word_index['excited']"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "944"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OryYy5fQdvWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ec110010-bb0a-4bc2-e030-44f7df4abca8"
      },
      "source": [
        "print(tus[0])\n",
        "print(tweets_pad[0])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "excited\n",
            "[944]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an1jxo2mLbdZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ce4b73ba-6729-45af-ff09-b2d2f3756253"
      },
      "source": [
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(quora_text2) \n",
        "sequences = tokenizer.texts_to_sequences(quora_text2)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.',len(word_index))\n",
        "quora_maxlen = max([len(s.split()) for s in quora_text])\n",
        "quora_pad = pad_sequences(sequences, maxlen=quora_maxlen)\n",
        "print(quora_pad.shape) \n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found %s unique tokens. 15507\n",
            "(13842, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xt_6VNFUo77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "081e7d33-5e99-43cf-8e31-7def6d65e364"
      },
      "source": [
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(wiki_text2) \n",
        "sequences = tokenizer.texts_to_sequences(wiki_text2)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.',len(word_index))\n",
        "wiki_maxlen = max([len(s.split()) for s in wiki_text])\n",
        "wiki_pad = pad_sequences(sequences, maxlen=wiki_maxlen)\n",
        "print(wiki_pad.shape) "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found %s unique tokens. 32848\n",
            "(10204, 1250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpkjVWOtN8vB",
        "colab_type": "text"
      },
      "source": [
        "# Making dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5D0WqO5MStA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_df=pd.concat([pd.DataFrame(twitter_text),pd.DataFrame(tweets_pad)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULvzSMJGOJ_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora_df=pd.concat([pd.DataFrame(quora_text),pd.DataFrame(quora_pad)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKlupcqtORky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_df=pd.concat([pd.DataFrame(wiki_text),pd.DataFrame(wiki_pad)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m1L_mLjdGsj",
        "colab_type": "text"
      },
      "source": [
        "# Co training Iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWqSFVA2fNB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora=pd.read_excel('quora.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiDxOFyBfMxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter = pd.read_excel('twitter2_data.xlsx')\n",
        "twitter.drop(['level_0','index','Unnamed: 0'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCOtKwh2fMZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki=pd.read_excel('wiki.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7EoDeBYV_p4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "867fe9b2-7ee1-421f-8823-3b83412ebf5a"
      },
      "source": [
        "print('Twitter:',twitter.shape)\n",
        "print('Wiki:',wiki.shape)\n",
        "print('Quora:',quora.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Twitter: (10841, 4)\n",
            "Wiki: (10204, 4)\n",
            "Quora: (13842, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld-LtfLjWasr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "72f530fb-6e62-4b63-ea4e-4f132fb7e69a"
      },
      "source": [
        "np.mean(twitter['label'])\n",
        "np.mean(wiki['label'])\n",
        "np.mean(quora['label'])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29186533737899145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lk2BBOqfnsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora.columns = twitter.columns\n",
        "wiki.columns = twitter.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG3JXeF9MF6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d83bc318-808c-4fa1-f88f-fec546b19d83"
      },
      "source": [
        "%%time\n",
        "twitter_text = TextPreprocessor(n_jobs=-1).transform(twitter['text'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 55s, sys: 970 ms, total: 1min 56s\n",
            "Wall time: 1min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr2Ueix7Oc0L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "88b2e758-19c1-4e40-808c-b78213264596"
      },
      "source": [
        "%%time\n",
        "quora_text = TextPreprocessor(n_jobs=-1).transform(quora['text'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 16s, sys: 690 ms, total: 2min 17s\n",
            "Wall time: 2min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMoeqvBbOk7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "79bbaf9c-5b9f-4cca-9903-604fb4ae8196"
      },
      "source": [
        "%%time\n",
        "wiki_text = TextPreprocessor(n_jobs=-1).transform(wiki['text'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 58s, sys: 1.17 s, total: 3min\n",
            "Wall time: 3min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYgEol2WOGSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter['clean_text'] = twitter_text\n",
        "wiki['clean_text'] = wiki_text\n",
        "quora['clean_text'] = quora_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33ETnvkDeiYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LoadGlove():\n",
        "  print('Loading word vectors')\n",
        "  embeddings_index = {}\n",
        "  f = open('/content/drive/My Drive/minor2/twitter2/glove.6B.100d.txt','r')\n",
        "  for line in tqdm(f):\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      values = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = values\n",
        "  f.close()\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "  return embeddings_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRgjHZ5AeNuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_embedding_matrix(tokenizer,emb_mean,emb_std,embeddings_index):\n",
        "  embedding_dim = 100\n",
        "  word_index = tokenizer.word_index\n",
        "  num_words = len(word_index)+1\n",
        "  embedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None: \n",
        "          embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InR-urZ_UY3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def emb_mean_std(embeddings_index):\n",
        "  all_embs = np.stack(embeddings_index.values())\n",
        "  emb_mean = all_embs.mean() # Calculate mean\n",
        "  emb_std = all_embs.std() # Calculate standard deviation\n",
        "  return emb_mean,emb_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygRM7mUnl0M4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_fun(text):\n",
        "  text2=[]\n",
        "  for i in text:\n",
        "        lst=i.split()\n",
        "        text2.append(lst)\n",
        "  return text2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQcPVWVCdZe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CoTrain(Train,Test,embeddings_index,mean,std):\n",
        "  emb_mean,emb_std = mean,std\n",
        "\n",
        "  train_text = Train['clean_text']\n",
        "  train_labels = Train['label']\n",
        "  test_text = Test['clean_text']\n",
        "  test_labels = Test['label']\n",
        "\n",
        "  train_text2 = split_fun(train_text)\n",
        "  test_text2 = split_fun(train_test)\n",
        "\n",
        "  tokenizer = Tokenizer() \n",
        "  tokenizer.fit_on_texts(train_text2)\n",
        "\n",
        "  train_sequences = tokenizer.texts_to_sequences(train_text2)\n",
        "  test_sequences =  tokenizer.texts_to_sequences(train_test2)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  #print('Found %s unique tokens.',len(word_index))\n",
        "\n",
        "  maxlength = max([len(s.split()) for s in train_text+test_text])\n",
        "  \n",
        "  train_pad = pad_sequences(train_sequences, maxlen=maxlength)\n",
        "  test_pad =  pad_sequences(test_sequences, maxlen=maxlength)\n",
        "\n",
        "  Train_df = pd.concat([pd.DataFrame(train_text),pd.DataFrame(train_pad)],axis=1)\n",
        "  Test_df =  pd.concat([pd.DataFrame(test_text),pd.DataFrame(test_pad)],axis=1) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ulOD5kzW4Wc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "f4cbd46a-fb70-4225-cc39-5b963ef090e5"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "embeddings_index = LoadGlove()\n",
        "emb_mean,emb_std = emb_mean_std(embeddings_index)\n",
        "x = twitter\n",
        "yl = quora.sample(frac=0.2)\n",
        "yu = quora.drop(list(yl.index))\n",
        "K = 5\n",
        "r = len(yl)//5\n",
        "for k in range(K+1):\n",
        "  #print(x.shape)\n",
        "  #CoTrain(x,yu,embeddings_index,emb_mean,emb_std)\n",
        "  z = yl.iloc[:r,:]\n",
        "  yl = yl.drop(list(z.index))\n",
        "  x =  pd.concat([x,z],axis=0)\n",
        "  shuffle(x)\n",
        "  x.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10841, 3)\n",
            "(2215, 3)\n",
            "(11394, 3)\n",
            "(1662, 3)\n",
            "(11947, 3)\n",
            "(1109, 3)\n",
            "(12500, 3)\n",
            "(556, 3)\n",
            "(13053, 3)\n",
            "(3, 3)\n",
            "(13606, 3)\n",
            "(0, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnkbigXoe51l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}