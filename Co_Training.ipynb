{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Co_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UjLUIozFNo99",
        "twRDck_4Nu7_",
        "krTAfaj8NJ2J",
        "qWMl6ujeNQP8",
        "SYi4pRdYPJ-F",
        "9nGNDI5hNYBz"
      ],
      "toc_visible": true,
      "mount_file_id": "1HKNpo5eSHoNZpz4XY99v38qFX0UyF-qy",
      "authorship_tag": "ABX9TyPwZH3pkz9kmwk19AJcGE+a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tushitgarg/Hate-Speech-Content-Moderation/blob/master/Co_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjLUIozFNo99",
        "colab_type": "text"
      },
      "source": [
        "# Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEXHHI2D-qwp",
        "colab_type": "code",
        "outputId": "80e7062f-3023-4706-9883-04a0a55da9fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "import pprint as pp\n",
        "! pip install normalise\n",
        "import nltk\n",
        "nltk.download('names')\n",
        "nltk.download('brown')\n",
        "\n",
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "\n",
        "import string\n",
        "import spacy \n",
        "import en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from normalise import normalise\n",
        "import re\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GRU, SimpleRNN\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras import layers\n",
        "from keras.initializers import Constant\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: normalise in /usr/local/lib/python3.6/dist-packages (0.1.8)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from normalise) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from normalise) (0.22.2.post1)\n",
            "Requirement already satisfied: roman in /usr/local/lib/python3.6/dist-packages (from normalise) (3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->normalise) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->normalise) (0.15.1)\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twRDck_4Nu7_",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILunTKA5JcZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,\n",
        "                 variety=\"BrE\",\n",
        "                 user_abbrevs={},\n",
        "                 n_jobs=1):\n",
        "        \"\"\"\n",
        "        Text preprocessing transformer includes steps:\n",
        "            1. Text normalization\n",
        "            2. Punctuation removal\n",
        "            3. Stop words removal\n",
        "            4. Lemmatization\n",
        "        \n",
        "        variety - format of date (AmE - american type, BrE - british format) \n",
        "        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n",
        "        n_jobs - parallel jobs to run\n",
        "        \"\"\"\n",
        "        self.variety = variety\n",
        "        self.user_abbrevs = user_abbrevs\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, *_):\n",
        "        X_copy = X.copy()\n",
        "        return X_copy.apply(self._preprocess_text)\n",
        "\n",
        "    def _preprocess_part(self, part):\n",
        "        return part.apply(self._preprocess_text)\n",
        "\n",
        "    def _preprocess_text(self, text):\n",
        "        text=self._clean_text(text)\n",
        "        normalized_text = self._normalize(text)\n",
        "        doc = nlp(normalized_text)\n",
        "        removed_punct = self._remove_punct(doc)\n",
        "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
        "        return self._lemmatize(removed_stop_words)\n",
        "\n",
        "    def _normalize(self, text):\n",
        "        # some issues in normalise package\n",
        "        try:\n",
        "            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def _remove_punct(self, doc):\n",
        "        return [t for t in doc if t.text not in string.punctuation]\n",
        "\n",
        "    def _remove_stop_words(self, doc):\n",
        "        return [t for t in doc if not t.is_stop]\n",
        "\n",
        "    def _lemmatize(self, doc):\n",
        "        return ' '.join([t.lemma_ for t in doc])\n",
        "    \n",
        "    \n",
        "    def _clean_text(self,text):\n",
        "      replace_1 = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
        "      replace_2 = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "      try:\n",
        "        text=re.sub(r\"http\\S+\", \"\", text)\n",
        "      except:\n",
        "        print(text)\n",
        "      text = replace_1.sub(\"\", text)\n",
        "      text = replace_2.sub(\" \", text)\n",
        "      text=re.sub('\\s+',' ',text)\n",
        "      return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZTA2ev__aWk",
        "colab_type": "code",
        "outputId": "3cc9cd20-ad0f-47a4-a1bd-fb294e60b189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCYubLAJ_m-Q",
        "colab_type": "code",
        "outputId": "8c636f9a-3ae3-4047-de4e-0443de8585de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd '/content/drive/My Drive/minor2/Hate-Speech-Content-Moderation/'"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/minor2/Hate-Speech-Content-Moderation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EomhQ_QnM_EE",
        "colab_type": "text"
      },
      "source": [
        "# Extracting text and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbStT9jZK9kV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_text=twitter['text']\n",
        "twitter_labels=twitter['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLHRCUPuK9pA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora_text=quora['text']\n",
        "quora_labels = quora['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dkkch82K9vW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_text=wiki['text']\n",
        "wiki_labels=wiki['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWMl6ujeNQP8",
        "colab_type": "text"
      },
      "source": [
        "# Splitting the text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSY5-8CVJxf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_text2=[]\n",
        "for i in twitter_text:\n",
        "      lst=i.split()\n",
        "      twitter_text2.append(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KrsdVX2KFqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora_text2=[]\n",
        "for i in quora_text:\n",
        "      lst=i.split()\n",
        "      quora_text2.append(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRZRH1nHKF-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_text2=[]\n",
        "for i in wiki_text:\n",
        "      lst=i.split()\n",
        "      wiki_text2.append(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nGNDI5hNYBz",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizing and padding the texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt5eieQxKQlW",
        "colab_type": "code",
        "outputId": "f3df9cf1-14ac-45d0-dc54-553de6623142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(twitter_text2) \n",
        "tus = [\"excited\"]\n",
        "sequences = tokenizer.texts_to_sequences(tus)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.',len(word_index))\n",
        "#twitter_maxlen = max([len(s.split()) for s in ' '.join(tus)])\n",
        "tweets_pad = pad_sequences(sequences, maxlen=1)\n",
        "print(tweets_pad.shape) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found %s unique tokens. 14884\n",
            "(1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZfEZoKJiu8Y",
        "colab_type": "code",
        "outputId": "86431574-9e6f-4227-c4e9-01a5bb2e48a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "word_index['excited']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "944"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OryYy5fQdvWN",
        "colab_type": "code",
        "outputId": "ec110010-bb0a-4bc2-e030-44f7df4abca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tus[0])\n",
        "print(tweets_pad[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "excited\n",
            "[944]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an1jxo2mLbdZ",
        "colab_type": "code",
        "outputId": "ce4b73ba-6729-45af-ff09-b2d2f3756253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(quora_text2) \n",
        "sequences = tokenizer.texts_to_sequences(quora_text2)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.',len(word_index))\n",
        "quora_maxlen = max([len(s.split()) for s in quora_text])\n",
        "quora_pad = pad_sequences(sequences, maxlen=quora_maxlen)\n",
        "print(quora_pad.shape) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found %s unique tokens. 15507\n",
            "(13842, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xt_6VNFUo77",
        "colab_type": "code",
        "outputId": "081e7d33-5e99-43cf-8e31-7def6d65e364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(wiki_text2) \n",
        "sequences = tokenizer.texts_to_sequences(wiki_text2)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.',len(word_index))\n",
        "wiki_maxlen = max([len(s.split()) for s in wiki_text])\n",
        "wiki_pad = pad_sequences(sequences, maxlen=wiki_maxlen)\n",
        "print(wiki_pad.shape) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found %s unique tokens. 32848\n",
            "(10204, 1250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpkjVWOtN8vB",
        "colab_type": "text"
      },
      "source": [
        "# Making dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5D0WqO5MStA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_df=pd.concat([pd.DataFrame(twitter_text),pd.DataFrame(tweets_pad)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULvzSMJGOJ_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora_df=pd.concat([pd.DataFrame(quora_text),pd.DataFrame(quora_pad)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKlupcqtORky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_df=pd.concat([pd.DataFrame(wiki_text),pd.DataFrame(wiki_pad)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkkQzcBjzulg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history,name_of_fig):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    f=plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    f.savefig(name_of_fig, bbox_inches='tight', dpi=400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m1L_mLjdGsj",
        "colab_type": "text"
      },
      "source": [
        "# Co training Iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWqSFVA2fNB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora=pd.read_excel('quora.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiDxOFyBfMxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter = pd.read_excel('twitter2_data.xlsx')\n",
        "twitter.drop(['level_0','index','Unnamed: 0'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCOtKwh2fMZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki=pd.read_excel('wiki.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7EoDeBYV_p4",
        "colab_type": "code",
        "outputId": "64d82396-d69a-4ef3-9195-b8b34259cd48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print('Twitter:',twitter.shape)\n",
        "print('Wiki:',wiki.shape)\n",
        "print('Quora:',quora.shape)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Twitter: (10841, 3)\n",
            "Wiki: (10204, 3)\n",
            "Quora: (13842, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld-LtfLjWasr",
        "colab_type": "code",
        "outputId": "f79f7007-a3ed-42c2-ec7a-72941e93a341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.mean(twitter['label'])\n",
        "np.mean(wiki['label'])\n",
        "np.mean(quora['label'])"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29186533737899145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lk2BBOqfnsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora.columns = twitter.columns\n",
        "wiki.columns = twitter.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG3JXeF9MF6j",
        "colab_type": "code",
        "outputId": "4f9a811f-f007-429c-a6e1-7c7f3d89a82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "twitter_text = TextPreprocessor(n_jobs=-1).transform(twitter['text'])"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min, sys: 806 ms, total: 2min 1s\n",
            "Wall time: 2min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr2Ueix7Oc0L",
        "colab_type": "code",
        "outputId": "f06ec277-8541-4f5e-cf22-5b78d59c0839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "quora_text = TextPreprocessor(n_jobs=-1).transform(quora['text'])"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 27s, sys: 587 ms, total: 2min 27s\n",
            "Wall time: 2min 28s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMoeqvBbOk7P",
        "colab_type": "code",
        "outputId": "d4bdac5d-1041-4fe9-a8f4-54b042ab7d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "wiki_text = TextPreprocessor(n_jobs=-1).transform(wiki['text'])"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 7s, sys: 977 ms, total: 3min 8s\n",
            "Wall time: 3min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYgEol2WOGSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter['clean_text'] = twitter_text\n",
        "wiki['clean_text'] = wiki_text\n",
        "quora['clean_text'] = quora_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33ETnvkDeiYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LoadGlove():\n",
        "  print('Loading word vectors....')\n",
        "  embeddings_index = {}\n",
        "  f = open('/content/drive/My Drive/minor2/twitter2/glove.6B.100d.txt','r')\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      values = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = values\n",
        "  f.close()\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "  return embeddings_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRgjHZ5AeNuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_embedding_matrix(tokenizer,emb_mean,emb_std,embeddings_index):\n",
        "  print('make_embedding_matrix function......')\n",
        "  embedding_dim = 100\n",
        "  word_index = tokenizer.word_index\n",
        "  num_words = len(word_index)+1\n",
        "  embedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None: \n",
        "          embedding_matrix[i] = embedding_vector\n",
        "  print(\"emb matrix shape:\",embedding_matrix.shape)\n",
        "  return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InR-urZ_UY3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def emb_mean_std(embeddings_index):\n",
        "  all_embs = np.stack(embeddings_index.values())\n",
        "  emb_mean = all_embs.mean() # Calculate mean\n",
        "  emb_std = all_embs.std() # Calculate standard deviation\n",
        "  return emb_mean,emb_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygRM7mUnl0M4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_fun(text):\n",
        "  print(\"splitting text into list....\")\n",
        "  text2=[]\n",
        "  for i in text:\n",
        "        lst=i.split()\n",
        "        text2.append(lst)\n",
        "  return text2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biFIyZiq0R4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_lstm_model(word_index,embedding_matrix,maxlength):\n",
        "  model = Sequential()\n",
        "  embedding_layer = Embedding(len(word_index)+1,\n",
        "                              100,\n",
        "                              embeddings_initializer = Constant(embedding_matrix),\n",
        "                              input_length = maxlength,\n",
        "                              trainable=False)\n",
        "  model.add(embedding_layer)\n",
        "  model.add(LSTM(units=100))\n",
        "  model.add(Dropout(.3))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  #model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOMzwkF917-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_data(model,Test_df):\n",
        "  print(\"Predicting.......\")\n",
        "  #print(Test_df.shape)\n",
        "  y_classes=model.predict_classes(Test_df.iloc[:,1:])\n",
        "  #y_test = y_test.astype(int)\n",
        "  return y_classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvxRNa76226b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_scores(y_test, y_classes):\n",
        "  # accuracy: (tp + tn) / (p + n)\n",
        "  accuracy = accuracy_score(y_test, y_classes)\n",
        "  print('Accuracy:{}\\n'.format(accuracy))\n",
        "  # precision tp / (tp + fp)\n",
        "  precision = precision_score(y_test, y_classes)\n",
        "  print('Precision:{}\\n'.format(precision))\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(y_test, y_classes)\n",
        "  print('Recall:{}\\n'.format(recall))\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(y_test, y_classes)\n",
        "  print('F1 score:{}\\n'.format(f1))\n",
        "  matrix = confusion_matrix(y_test, y_classes)\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQcPVWVCdZe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CoTrain(Train,Test,embeddings_index,mean,std):\n",
        "  emb_mean,emb_std = mean,std\n",
        "\n",
        "  train_text = Train['clean_text'].values\n",
        "  train_labels = Train['label'].values\n",
        "  test_text = Test['clean_text'].values\n",
        "  test_labels = Test['label'].values\n",
        "\n",
        "  #print('len of test_text:',len(test_text))\n",
        "\n",
        "  train_text2 = split_fun(train_text)\n",
        "  test_text2 = split_fun(test_text)\n",
        "\n",
        "  #print('len of test_text2:',len(test_text2))\n",
        "\n",
        "  tokenizer = Tokenizer() \n",
        "  tokenizer.fit_on_texts(train_text2)\n",
        "\n",
        "  train_sequences = tokenizer.texts_to_sequences(train_text2)\n",
        "  test_sequences =  tokenizer.texts_to_sequences(test_text2)\n",
        "\n",
        "  #print('len of test_sequences:',len(test_sequences))\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  #print('Found %s unique tokens.',len(word_index))\n",
        "  maxlength = 100\n",
        "  #maxlength = max([len(s.split()) for s in train_text+test_text])\n",
        "  \n",
        "  train_pad = pad_sequences(train_sequences, maxlen=maxlength)\n",
        "  test_pad =  pad_sequences(test_sequences, maxlen=maxlength)\n",
        "\n",
        "  #print('len of test_pad:',len(test_pad))\n",
        "\n",
        "  #print(test_pad[29])\n",
        "\n",
        "  Train_df = pd.concat([pd.DataFrame(train_text),pd.DataFrame(train_pad)],axis=1)\n",
        "  #print(pd.DataFrame(test_text))\n",
        "  Test_df =  pd.concat([pd.DataFrame(test_text),pd.DataFrame(test_pad)],axis=1) \n",
        "\n",
        "  #print(Train_df.shape)\n",
        "  #print(Test_df.shape)\n",
        "  embedding_matrix = make_embedding_matrix(tokenizer,emb_mean,emb_std,embeddings_index)\n",
        "\n",
        "  lstm_model = make_lstm_model(word_index,embedding_matrix,maxlength)\n",
        "\n",
        "  print(\"Train Test Split data.......\")\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X_train, X_val ,y_train, y_val = train_test_split(Train_df, train_labels, test_size = 0.30, random_state = 1)\n",
        "  \n",
        "  \n",
        "  print('Training.....')\n",
        "  history=lstm_model.fit(X_train.iloc[:,1:], y_train, batch_size=256,epochs=5,validation_data=(X_val.iloc[:,1:],y_val),verbose=True)\n",
        "\n",
        "  predicted_classes = predict_data(lstm_model,Test_df)\n",
        "  print_scores(test_labels,predicted_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "985NLh4G83Mv",
        "colab_type": "code",
        "outputId": "72d89d95-b266-4c4e-b128-18f779e11dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "embeddings_index = LoadGlove()\n",
        "emb_mean,emb_std = emb_mean_std(embeddings_index)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word vectors....\n",
            "Found 400001 word vectors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ulOD5kzW4Wc",
        "colab_type": "code",
        "outputId": "8e089705-8476-4b73-b89f-05ba5b269e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "#embeddings_index = LoadGlove()\n",
        "#emb_mean,emb_std = emb_mean_std(embeddings_index)\n",
        "x = twitter\n",
        "yl = quora.sample(frac=0.2)\n",
        "yu = quora.drop(list(yl.index))\n",
        "K = 5\n",
        "r = len(yl)//5\n",
        "for k in range(K+1):\n",
        "  #print(x.shape)\n",
        "  CoTrain(x,yu,embeddings_index,emb_mean,emb_std)\n",
        "  z = yl.iloc[:r,:]\n",
        "  yl = yl.drop(list(z.index))\n",
        "  x =  pd.concat([x,z],axis=0)\n",
        "  shuffle(x)\n",
        "  x.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (14885, 100)\n",
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_37 (Embedding)     (None, 100, 100)          1488500   \n",
            "_________________________________________________________________\n",
            "lstm_37 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,569,001\n",
            "Trainable params: 80,501\n",
            "Non-trainable params: 1,488,500\n",
            "_________________________________________________________________\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Train on 7588 samples, validate on 3253 samples\n",
            "Epoch 1/5\n",
            "7588/7588 [==============================] - 18s 2ms/step - loss: 0.5351 - accuracy: 0.7438 - val_loss: 0.4596 - val_accuracy: 0.8140\n",
            "Epoch 2/5\n",
            "7588/7588 [==============================] - 18s 2ms/step - loss: 0.4397 - accuracy: 0.8142 - val_loss: 0.4119 - val_accuracy: 0.8245\n",
            "Epoch 3/5\n",
            "7588/7588 [==============================] - 18s 2ms/step - loss: 0.4166 - accuracy: 0.8229 - val_loss: 0.3999 - val_accuracy: 0.8275\n",
            "Epoch 4/5\n",
            "7588/7588 [==============================] - 17s 2ms/step - loss: 0.3948 - accuracy: 0.8322 - val_loss: 0.3907 - val_accuracy: 0.8358\n",
            "Epoch 5/5\n",
            "7588/7588 [==============================] - 17s 2ms/step - loss: 0.3890 - accuracy: 0.8346 - val_loss: 0.3844 - val_accuracy: 0.8405\n",
            "(11074, 101)\n",
            "Accuracy:0.7260249232436338\n",
            "\n",
            "Precision:0.7869822485207101\n",
            "\n",
            "Recall:0.08240396530359356\n",
            "\n",
            "F1 score:0.14918676388109928\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7774   72]\n",
            " [2962  266]]\n",
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (15670, 100)\n",
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_38 (Embedding)     (None, 100, 100)          1567000   \n",
            "_________________________________________________________________\n",
            "lstm_38 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,647,501\n",
            "Trainable params: 80,501\n",
            "Non-trainable params: 1,567,000\n",
            "_________________________________________________________________\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Train on 7975 samples, validate on 3419 samples\n",
            "Epoch 1/5\n",
            "7975/7975 [==============================] - 19s 2ms/step - loss: 0.5258 - accuracy: 0.7582 - val_loss: 0.4606 - val_accuracy: 0.8008\n",
            "Epoch 2/5\n",
            "7975/7975 [==============================] - 18s 2ms/step - loss: 0.4302 - accuracy: 0.8147 - val_loss: 0.4293 - val_accuracy: 0.8163\n",
            "Epoch 3/5\n",
            "7975/7975 [==============================] - 19s 2ms/step - loss: 0.4089 - accuracy: 0.8261 - val_loss: 0.4093 - val_accuracy: 0.8242\n",
            "Epoch 4/5\n",
            "7975/7975 [==============================] - 18s 2ms/step - loss: 0.3928 - accuracy: 0.8332 - val_loss: 0.4002 - val_accuracy: 0.8274\n",
            "Epoch 5/5\n",
            "7975/7975 [==============================] - 19s 2ms/step - loss: 0.3796 - accuracy: 0.8414 - val_loss: 0.4003 - val_accuracy: 0.8283\n",
            "(11074, 101)\n",
            "Accuracy:0.768466678706881\n",
            "\n",
            "Precision:0.775290215588723\n",
            "\n",
            "Recall:0.2896530359355638\n",
            "\n",
            "F1 score:0.4217410915651782\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7575  271]\n",
            " [2293  935]]\n",
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (16276, 100)\n",
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_39 (Embedding)     (None, 100, 100)          1627600   \n",
            "_________________________________________________________________\n",
            "lstm_39 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,708,101\n",
            "Trainable params: 80,501\n",
            "Non-trainable params: 1,627,600\n",
            "_________________________________________________________________\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Train on 8362 samples, validate on 3585 samples\n",
            "Epoch 1/5\n",
            "8362/8362 [==============================] - 20s 2ms/step - loss: 0.5241 - accuracy: 0.7570 - val_loss: 0.4500 - val_accuracy: 0.8081\n",
            "Epoch 2/5\n",
            "8362/8362 [==============================] - 20s 2ms/step - loss: 0.4437 - accuracy: 0.8094 - val_loss: 0.4091 - val_accuracy: 0.8259\n",
            "Epoch 3/5\n",
            "8362/8362 [==============================] - 19s 2ms/step - loss: 0.4173 - accuracy: 0.8203 - val_loss: 0.3953 - val_accuracy: 0.8318\n",
            "Epoch 4/5\n",
            "8362/8362 [==============================] - 19s 2ms/step - loss: 0.4003 - accuracy: 0.8280 - val_loss: 0.3804 - val_accuracy: 0.8349\n",
            "Epoch 5/5\n",
            "8362/8362 [==============================] - 19s 2ms/step - loss: 0.3899 - accuracy: 0.8331 - val_loss: 0.3815 - val_accuracy: 0.8385\n",
            "(11074, 101)\n",
            "Accuracy:0.8019685750406357\n",
            "\n",
            "Precision:0.7804878048780488\n",
            "\n",
            "Recall:0.44609665427509293\n",
            "\n",
            "F1 score:0.5677114133648729\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7441  405]\n",
            " [1788 1440]]\n",
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (16850, 100)\n",
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_40 (Embedding)     (None, 100, 100)          1685000   \n",
            "_________________________________________________________________\n",
            "lstm_40 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,765,501\n",
            "Trainable params: 80,501\n",
            "Non-trainable params: 1,685,000\n",
            "_________________________________________________________________\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Train on 8750 samples, validate on 3750 samples\n",
            "Epoch 1/5\n",
            "8750/8750 [==============================] - 21s 2ms/step - loss: 0.5300 - accuracy: 0.7485 - val_loss: 0.4672 - val_accuracy: 0.7965\n",
            "Epoch 2/5\n",
            "8750/8750 [==============================] - 20s 2ms/step - loss: 0.4425 - accuracy: 0.8085 - val_loss: 0.4292 - val_accuracy: 0.8101\n",
            "Epoch 3/5\n",
            "8750/8750 [==============================] - 20s 2ms/step - loss: 0.4154 - accuracy: 0.8242 - val_loss: 0.4195 - val_accuracy: 0.8165\n",
            "Epoch 4/5\n",
            "8750/8750 [==============================] - 21s 2ms/step - loss: 0.3985 - accuracy: 0.8329 - val_loss: 0.4081 - val_accuracy: 0.8243\n",
            "Epoch 5/5\n",
            "8750/8750 [==============================] - 20s 2ms/step - loss: 0.3866 - accuracy: 0.8403 - val_loss: 0.4207 - val_accuracy: 0.8205\n",
            "(11074, 101)\n",
            "Accuracy:0.7585335018963337\n",
            "\n",
            "Precision:0.8569587628865979\n",
            "\n",
            "Recall:0.20600991325898388\n",
            "\n",
            "F1 score:0.33216783216783213\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7735  111]\n",
            " [2563  665]]\n",
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (17396, 100)\n",
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_41 (Embedding)     (None, 100, 100)          1739600   \n",
            "_________________________________________________________________\n",
            "lstm_41 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,820,101\n",
            "Trainable params: 80,501\n",
            "Non-trainable params: 1,739,600\n",
            "_________________________________________________________________\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Train on 9137 samples, validate on 3916 samples\n",
            "Epoch 1/5\n",
            "9137/9137 [==============================] - 25s 3ms/step - loss: 0.5206 - accuracy: 0.7614 - val_loss: 0.4339 - val_accuracy: 0.8133\n",
            "Epoch 2/5\n",
            "9137/9137 [==============================] - 22s 2ms/step - loss: 0.4461 - accuracy: 0.8121 - val_loss: 0.4163 - val_accuracy: 0.8205\n",
            "Epoch 3/5\n",
            "9137/9137 [==============================] - 21s 2ms/step - loss: 0.4240 - accuracy: 0.8185 - val_loss: 0.4016 - val_accuracy: 0.8289\n",
            "Epoch 4/5\n",
            "9137/9137 [==============================] - 21s 2ms/step - loss: 0.4072 - accuracy: 0.8213 - val_loss: 0.3965 - val_accuracy: 0.8327\n",
            "Epoch 5/5\n",
            "9137/9137 [==============================] - 21s 2ms/step - loss: 0.3971 - accuracy: 0.8323 - val_loss: 0.3949 - val_accuracy: 0.8292\n",
            "(11074, 101)\n",
            "Accuracy:0.7771356330142677\n",
            "\n",
            "Precision:0.8398926654740608\n",
            "\n",
            "Recall:0.2908921933085502\n",
            "\n",
            "F1 score:0.43212149102623104\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7667  179]\n",
            " [2289  939]]\n",
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (17934, 100)\n",
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_42 (Embedding)     (None, 100, 100)          1793400   \n",
            "_________________________________________________________________\n",
            "lstm_42 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,873,901\n",
            "Trainable params: 80,501\n",
            "Non-trainable params: 1,793,400\n",
            "_________________________________________________________________\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Train on 9524 samples, validate on 4082 samples\n",
            "Epoch 1/5\n",
            "9524/9524 [==============================] - 22s 2ms/step - loss: 0.5054 - accuracy: 0.7659 - val_loss: 0.4618 - val_accuracy: 0.8013\n",
            "Epoch 2/5\n",
            "9524/9524 [==============================] - 22s 2ms/step - loss: 0.4339 - accuracy: 0.8128 - val_loss: 0.4222 - val_accuracy: 0.8138\n",
            "Epoch 3/5\n",
            "9524/9524 [==============================] - 22s 2ms/step - loss: 0.4163 - accuracy: 0.8226 - val_loss: 0.4293 - val_accuracy: 0.8163\n",
            "Epoch 4/5\n",
            "9524/9524 [==============================] - 22s 2ms/step - loss: 0.4026 - accuracy: 0.8292 - val_loss: 0.4068 - val_accuracy: 0.8234\n",
            "Epoch 5/5\n",
            "9524/9524 [==============================] - 22s 2ms/step - loss: 0.3880 - accuracy: 0.8327 - val_loss: 0.4055 - val_accuracy: 0.8204\n",
            "(11074, 101)\n",
            "Accuracy:0.7939317319848294\n",
            "\n",
            "Precision:0.8540419161676647\n",
            "\n",
            "Recall:0.3534696406443618\n",
            "\n",
            "F1 score:0.5\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7651  195]\n",
            " [2087 1141]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSe7PUrYX-rO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "483f0538-ec66-44d2-a23e-fad1cd2b6f5c"
      },
      "source": [
        ""
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-85944efcb991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'embeddings_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcp8NPIIasEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}