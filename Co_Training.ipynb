{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Co_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UjLUIozFNo99",
        "twRDck_4Nu7_",
        "krTAfaj8NJ2J",
        "qWMl6ujeNQP8",
        "SYi4pRdYPJ-F",
        "9nGNDI5hNYBz"
      ],
      "toc_visible": true,
      "mount_file_id": "1HKNpo5eSHoNZpz4XY99v38qFX0UyF-qy",
      "authorship_tag": "ABX9TyOcnRPu81FegF1eD3HgomLP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tushitgarg/Hate-Speech-Content-Moderation/blob/master/Co_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjLUIozFNo99",
        "colab_type": "text"
      },
      "source": [
        "# Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEXHHI2D-qwp",
        "colab_type": "code",
        "outputId": "80e7062f-3023-4706-9883-04a0a55da9fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "import pprint as pp\n",
        "! pip install normalise\n",
        "import nltk\n",
        "nltk.download('names')\n",
        "nltk.download('brown')\n",
        "\n",
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "\n",
        "import string\n",
        "import spacy \n",
        "import en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from normalise import normalise\n",
        "import re\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GRU, SimpleRNN\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras import layers\n",
        "from keras.initializers import Constant\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: normalise in /usr/local/lib/python3.6/dist-packages (0.1.8)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from normalise) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from normalise) (0.22.2.post1)\n",
            "Requirement already satisfied: roman in /usr/local/lib/python3.6/dist-packages (from normalise) (3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->normalise) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->normalise) (0.15.1)\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twRDck_4Nu7_",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILunTKA5JcZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,\n",
        "                 variety=\"BrE\",\n",
        "                 user_abbrevs={},\n",
        "                 n_jobs=1):\n",
        "        \"\"\"\n",
        "        Text preprocessing transformer includes steps:\n",
        "            1. Text normalization\n",
        "            2. Punctuation removal\n",
        "            3. Stop words removal\n",
        "            4. Lemmatization\n",
        "        \n",
        "        variety - format of date (AmE - american type, BrE - british format) \n",
        "        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n",
        "        n_jobs - parallel jobs to run\n",
        "        \"\"\"\n",
        "        self.variety = variety\n",
        "        self.user_abbrevs = user_abbrevs\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, *_):\n",
        "        X_copy = X.copy()\n",
        "        return X_copy.apply(self._preprocess_text)\n",
        "\n",
        "    def _preprocess_part(self, part):\n",
        "        return part.apply(self._preprocess_text)\n",
        "\n",
        "    def _preprocess_text(self, text):\n",
        "        text=self._clean_text(text)\n",
        "        normalized_text = self._normalize(text)\n",
        "        doc = nlp(normalized_text)\n",
        "        removed_punct = self._remove_punct(doc)\n",
        "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
        "        return self._lemmatize(removed_stop_words)\n",
        "\n",
        "    def _normalize(self, text):\n",
        "        # some issues in normalise package\n",
        "        try:\n",
        "            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def _remove_punct(self, doc):\n",
        "        return [t for t in doc if t.text not in string.punctuation]\n",
        "\n",
        "    def _remove_stop_words(self, doc):\n",
        "        return [t for t in doc if not t.is_stop]\n",
        "\n",
        "    def _lemmatize(self, doc):\n",
        "        return ' '.join([t.lemma_ for t in doc])\n",
        "    \n",
        "    \n",
        "    def _clean_text(self,text):\n",
        "      replace_1 = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
        "      replace_2 = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "      try:\n",
        "        text=re.sub(r\"http\\S+\", \"\", text)\n",
        "      except:\n",
        "        print(text)\n",
        "      text = replace_1.sub(\"\", text)\n",
        "      text = replace_2.sub(\" \", text)\n",
        "      text=re.sub('\\s+',' ',text)\n",
        "      return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZTA2ev__aWk",
        "colab_type": "code",
        "outputId": "3cc9cd20-ad0f-47a4-a1bd-fb294e60b189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCYubLAJ_m-Q",
        "colab_type": "code",
        "outputId": "8c636f9a-3ae3-4047-de4e-0443de8585de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd '/content/drive/My Drive/minor2/Hate-Speech-Content-Moderation/'"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/minor2/Hate-Speech-Content-Moderation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EomhQ_QnM_EE",
        "colab_type": "text"
      },
      "source": [
        "# Extracting text and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbStT9jZK9kV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_text=twitter['text']\n",
        "twitter_labels=twitter['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLHRCUPuK9pA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora_text=quora['text']\n",
        "quora_labels = quora['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dkkch82K9vW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_text=wiki['text']\n",
        "wiki_labels=wiki['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWMl6ujeNQP8",
        "colab_type": "text"
      },
      "source": [
        "# Splitting the text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSY5-8CVJxf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_text2=[]\n",
        "for i in twitter_text:\n",
        "      lst=i.split()\n",
        "      twitter_text2.append(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KrsdVX2KFqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora_text2=[]\n",
        "for i in quora_text:\n",
        "      lst=i.split()\n",
        "      quora_text2.append(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRZRH1nHKF-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_text2=[]\n",
        "for i in wiki_text:\n",
        "      lst=i.split()\n",
        "      wiki_text2.append(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nGNDI5hNYBz",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizing and padding the texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt5eieQxKQlW",
        "colab_type": "code",
        "outputId": "f3df9cf1-14ac-45d0-dc54-553de6623142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(twitter_text2) \n",
        "tus = [\"excited\"]\n",
        "sequences = tokenizer.texts_to_sequences(tus)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.',len(word_index))\n",
        "#twitter_maxlen = max([len(s.split()) for s in ' '.join(tus)])\n",
        "tweets_pad = pad_sequences(sequences, maxlen=1)\n",
        "print(tweets_pad.shape) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found %s unique tokens. 14884\n",
            "(1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZfEZoKJiu8Y",
        "colab_type": "code",
        "outputId": "86431574-9e6f-4227-c4e9-01a5bb2e48a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "word_index['excited']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "944"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OryYy5fQdvWN",
        "colab_type": "code",
        "outputId": "ec110010-bb0a-4bc2-e030-44f7df4abca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tus[0])\n",
        "print(tweets_pad[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "excited\n",
            "[944]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an1jxo2mLbdZ",
        "colab_type": "code",
        "outputId": "ce4b73ba-6729-45af-ff09-b2d2f3756253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(quora_text2) \n",
        "sequences = tokenizer.texts_to_sequences(quora_text2)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.',len(word_index))\n",
        "quora_maxlen = max([len(s.split()) for s in quora_text])\n",
        "quora_pad = pad_sequences(sequences, maxlen=quora_maxlen)\n",
        "print(quora_pad.shape) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found %s unique tokens. 15507\n",
            "(13842, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xt_6VNFUo77",
        "colab_type": "code",
        "outputId": "081e7d33-5e99-43cf-8e31-7def6d65e364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(wiki_text2) \n",
        "sequences = tokenizer.texts_to_sequences(wiki_text2)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.',len(word_index))\n",
        "wiki_maxlen = max([len(s.split()) for s in wiki_text])\n",
        "wiki_pad = pad_sequences(sequences, maxlen=wiki_maxlen)\n",
        "print(wiki_pad.shape) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found %s unique tokens. 32848\n",
            "(10204, 1250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpkjVWOtN8vB",
        "colab_type": "text"
      },
      "source": [
        "# Making dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5D0WqO5MStA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_df=pd.concat([pd.DataFrame(twitter_text),pd.DataFrame(tweets_pad)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULvzSMJGOJ_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora_df=pd.concat([pd.DataFrame(quora_text),pd.DataFrame(quora_pad)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKlupcqtORky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_df=pd.concat([pd.DataFrame(wiki_text),pd.DataFrame(wiki_pad)],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkkQzcBjzulg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history,name_of_fig):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    f=plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    f.savefig(name_of_fig, bbox_inches='tight', dpi=400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m1L_mLjdGsj",
        "colab_type": "text"
      },
      "source": [
        "# Co training Iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWqSFVA2fNB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora=pd.read_excel('quora.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiDxOFyBfMxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter = pd.read_excel('twitter2_data.xlsx')\n",
        "twitter.drop(['level_0','index','Unnamed: 0'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCOtKwh2fMZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki=pd.read_excel('wiki.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7EoDeBYV_p4",
        "colab_type": "code",
        "outputId": "64d82396-d69a-4ef3-9195-b8b34259cd48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print('Twitter:',twitter.shape)\n",
        "print('Wiki:',wiki.shape)\n",
        "print('Quora:',quora.shape)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Twitter: (10841, 3)\n",
            "Wiki: (10204, 3)\n",
            "Quora: (13842, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld-LtfLjWasr",
        "colab_type": "code",
        "outputId": "f79f7007-a3ed-42c2-ec7a-72941e93a341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.mean(twitter['label'])\n",
        "np.mean(wiki['label'])\n",
        "np.mean(quora['label'])"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29186533737899145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lk2BBOqfnsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quora.columns = twitter.columns\n",
        "wiki.columns = twitter.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG3JXeF9MF6j",
        "colab_type": "code",
        "outputId": "4f9a811f-f007-429c-a6e1-7c7f3d89a82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "twitter_text = TextPreprocessor(n_jobs=-1).transform(twitter['text'])"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min, sys: 806 ms, total: 2min 1s\n",
            "Wall time: 2min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr2Ueix7Oc0L",
        "colab_type": "code",
        "outputId": "f06ec277-8541-4f5e-cf22-5b78d59c0839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "quora_text = TextPreprocessor(n_jobs=-1).transform(quora['text'])"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 27s, sys: 587 ms, total: 2min 27s\n",
            "Wall time: 2min 28s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMoeqvBbOk7P",
        "colab_type": "code",
        "outputId": "d4bdac5d-1041-4fe9-a8f4-54b042ab7d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "wiki_text = TextPreprocessor(n_jobs=-1).transform(wiki['text'])"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 7s, sys: 977 ms, total: 3min 8s\n",
            "Wall time: 3min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYgEol2WOGSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter['clean_text'] = twitter_text\n",
        "wiki['clean_text'] = wiki_text\n",
        "quora['clean_text'] = quora_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33ETnvkDeiYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LoadGlove():\n",
        "  print('Loading word vectors....')\n",
        "  embeddings_index = {}\n",
        "  f = open('/content/drive/My Drive/minor2/twitter2/glove.6B.100d.txt','r')\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      values = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = values\n",
        "  f.close()\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "  return embeddings_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRgjHZ5AeNuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_embedding_matrix(tokenizer,emb_mean,emb_std,embeddings_index):\n",
        "  print('make_embedding_matrix function......')\n",
        "  embedding_dim = 100\n",
        "  word_index = tokenizer.word_index\n",
        "  num_words = len(word_index)+1\n",
        "  embedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None: \n",
        "          embedding_matrix[i] = embedding_vector\n",
        "  print(\"emb matrix shape:\",embedding_matrix.shape)\n",
        "  return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InR-urZ_UY3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def emb_mean_std(embeddings_index):\n",
        "  all_embs = np.stack(embeddings_index.values())\n",
        "  emb_mean = all_embs.mean() # Calculate mean\n",
        "  emb_std = all_embs.std() # Calculate standard deviation\n",
        "  return emb_mean,emb_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygRM7mUnl0M4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_fun(text):\n",
        "  print(\"splitting text into list....\")\n",
        "  text2=[]\n",
        "  for i in text:\n",
        "        lst=i.split()\n",
        "        text2.append(lst)\n",
        "  return text2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biFIyZiq0R4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_lstm_model(word_index,embedding_matrix,maxlength):\n",
        "  model = Sequential()\n",
        "  embedding_layer = Embedding(len(word_index)+1,\n",
        "                              100,\n",
        "                              embeddings_initializer = Constant(embedding_matrix),\n",
        "                              input_length = maxlength,\n",
        "                              trainable=False)\n",
        "  model.add(embedding_layer)\n",
        "  model.add(LSTM(units=100))\n",
        "  model.add(Dropout(.3))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  #model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOMzwkF917-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_data(model,Test_df):\n",
        "  print(\"Predicting.......\")\n",
        "  #print(Test_df.shape)\n",
        "  y_classes=model.predict_classes(Test_df.iloc[:,1:])\n",
        "  #y_test = y_test.astype(int)\n",
        "  return y_classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvxRNa76226b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_scores(y_test, y_classes):\n",
        "  scores = []\n",
        "  # accuracy: (tp + tn) / (p + n)\n",
        "  accuracy = accuracy_score(y_test, y_classes)\n",
        "  print('Accuracy:{}\\n'.format(accuracy))\n",
        "  # precision tp / (tp + fp)\n",
        "  precision = precision_score(y_test, y_classes)\n",
        "  print('Precision:{}\\n'.format(precision))\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(y_test, y_classes)\n",
        "  print('Recall:{}\\n'.format(recall))\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(y_test, y_classes)\n",
        "  print('F1 score:{}\\n'.format(f1))\n",
        "  matrix = confusion_matrix(y_test, y_classes)\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(matrix)\n",
        "  scores.append(accuracy)\n",
        "  scores.append(precision)\n",
        "  scores.append(recall)\n",
        "  scores.append(f1)\n",
        "  return scores\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQcPVWVCdZe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CoTrain(Train,Test,embeddings_index,mean,std):\n",
        "  emb_mean,emb_std = mean,std\n",
        "\n",
        "  train_text = Train['clean_text'].values\n",
        "  train_labels = Train['label'].values\n",
        "  test_text = Test['clean_text'].values\n",
        "  test_labels = Test['label'].values\n",
        "\n",
        "  #print('len of test_text:',len(test_text))\n",
        "\n",
        "  train_text2 = split_fun(train_text)\n",
        "  test_text2 = split_fun(test_text)\n",
        "\n",
        "  #print('len of test_text2:',len(test_text2))\n",
        "\n",
        "  tokenizer = Tokenizer() \n",
        "  tokenizer.fit_on_texts(train_text2)\n",
        "\n",
        "  train_sequences = tokenizer.texts_to_sequences(train_text2)\n",
        "  test_sequences =  tokenizer.texts_to_sequences(test_text2)\n",
        "\n",
        "  #print('len of test_sequences:',len(test_sequences))\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  #print('Found %s unique tokens.',len(word_index))\n",
        "  maxlength = 100\n",
        "  #maxlength = max([len(s.split()) for s in train_text+test_text])\n",
        "  \n",
        "  train_pad = pad_sequences(train_sequences, maxlen=maxlength)\n",
        "  test_pad =  pad_sequences(test_sequences, maxlen=maxlength)\n",
        "\n",
        "  #print('len of test_pad:',len(test_pad))\n",
        "\n",
        "  #print(test_pad[29])\n",
        "\n",
        "  Train_df = pd.concat([pd.DataFrame(train_text),pd.DataFrame(train_pad)],axis=1)\n",
        "  #print(pd.DataFrame(test_text))\n",
        "  Test_df =  pd.concat([pd.DataFrame(test_text),pd.DataFrame(test_pad)],axis=1) \n",
        "\n",
        "  #print(Train_df.shape)\n",
        "  #print(Test_df.shape)\n",
        "  embedding_matrix = make_embedding_matrix(tokenizer,emb_mean,emb_std,embeddings_index)\n",
        "\n",
        "  lstm_model = make_lstm_model(word_index,embedding_matrix,maxlength)\n",
        "\n",
        "  print(\"Train Test Split data.......\")\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X_train, X_val ,y_train, y_val = train_test_split(Train_df, train_labels, test_size = 0.30, random_state = 1)\n",
        "  \n",
        "  \n",
        "  print('Training.....')\n",
        "  history=lstm_model.fit(X_train.iloc[:,1:], y_train, batch_size=256,epochs=5,validation_data=(X_val.iloc[:,1:],y_val),verbose=False)\n",
        "  \n",
        "  predicted_classes = predict_data(lstm_model,Test_df)\n",
        "  return print_scores(test_labels,predicted_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "985NLh4G83Mv",
        "colab_type": "code",
        "outputId": "72d89d95-b266-4c4e-b128-18f779e11dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "embeddings_index = LoadGlove()\n",
        "emb_mean,emb_std = emb_mean_std(embeddings_index)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word vectors....\n",
            "Found 400001 word vectors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ulOD5kzW4Wc",
        "colab_type": "code",
        "outputId": "1e6da682-8c76-4645-fcd7-b1bb821abf5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "#embeddings_index = LoadGlove()\n",
        "#emb_mean,emb_std = emb_mean_std(embeddings_index)\n",
        "x = twitter\n",
        "yl = quora.sample(frac=0.2)\n",
        "yu = quora.drop(list(yl.index))\n",
        "K = 5\n",
        "r = len(yl)//5\n",
        "scores = []\n",
        "for k in range(K+1):\n",
        "  #print(x.shape)\n",
        "  score = CoTrain(x,yu,embeddings_index,emb_mean,emb_std)\n",
        "  print(score)\n",
        "  scores.append(score)\n",
        "  z = yl.iloc[:r,:]\n",
        "  yl = yl.drop(list(z.index))\n",
        "  x =  pd.concat([x,z],axis=0)\n",
        "  shuffle(x)\n",
        "  x.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (14885, 100)\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Predicting.......\n",
            "Accuracy:0.7309915116489073\n",
            "\n",
            "Precision:0.7785888077858881\n",
            "\n",
            "Recall:0.09975062344139651\n",
            "\n",
            "F1 score:0.1768444321635811\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7775   91]\n",
            " [2888  320]]\n",
            "[0.7309915116489073, 0.7785888077858881, 0.09975062344139651, 0.1768444321635811]\n",
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (15566, 100)\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Predicting.......\n",
            "Accuracy:0.7253025103846849\n",
            "\n",
            "Precision:0.8007246376811594\n",
            "\n",
            "Recall:0.06889027431421446\n",
            "\n",
            "F1 score:0.12686567164179105\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7811   55]\n",
            " [2987  221]]\n",
            "[0.7253025103846849, 0.8007246376811594, 0.06889027431421446, 0.12686567164179105]\n",
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (16231, 100)\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Predicting.......\n",
            "Accuracy:0.7741556799711035\n",
            "\n",
            "Precision:0.8240146654445463\n",
            "\n",
            "Recall:0.28023690773067333\n",
            "\n",
            "F1 score:0.4182367992556409\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7674  192]\n",
            " [2309  899]]\n",
            "[0.7741556799711035, 0.8240146654445463, 0.28023690773067333, 0.4182367992556409]\n",
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (16783, 100)\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Predicting.......\n",
            "Accuracy:0.7909517789416651\n",
            "\n",
            "Precision:0.8006734006734006\n",
            "\n",
            "Recall:0.3706359102244389\n",
            "\n",
            "F1 score:0.5067121244406564\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7570  296]\n",
            " [2019 1189]]\n",
            "[0.7909517789416651, 0.8006734006734006, 0.3706359102244389, 0.5067121244406564]\n",
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (17359, 100)\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Predicting.......\n",
            "Accuracy:0.7915838901932455\n",
            "\n",
            "Precision:0.8133704735376045\n",
            "\n",
            "Recall:0.3640897755610973\n",
            "\n",
            "F1 score:0.5030146425495263\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7598  268]\n",
            " [2040 1168]]\n",
            "[0.7915838901932455, 0.8133704735376045, 0.3640897755610973, 0.5030146425495263]\n",
            "splitting text into list....\n",
            "splitting text into list....\n",
            "make_embedding_matrix function......\n",
            "emb matrix shape: (17886, 100)\n",
            "Train Test Split data.......\n",
            "Training.....\n",
            "Predicting.......\n",
            "Accuracy:0.8254469929564746\n",
            "\n",
            "Precision:0.7615921214608125\n",
            "\n",
            "Recall:0.5785536159600998\n",
            "\n",
            "F1 score:0.6575730735163863\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7285  581]\n",
            " [1352 1856]]\n",
            "[0.8254469929564746, 0.7615921214608125, 0.5785536159600998, 0.6575730735163863]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSe7PUrYX-rO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "9ff5e97e-d5eb-4bba-f63c-2fd85e12e5ea"
      },
      "source": [
        "print(\"Average Accuracy:\",sum([i[0] for i in scores[2:]])/4)\n",
        "print(\"Average Precision:\",sum([i[1] for i in scores[2:]])/4)\n",
        "print(\"Average Recall:\",sum([i[2] for i in scores[2:]])/4)\n",
        "print(\"Average F1:\",sum([i[3] for i in scores][2:])/4)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.7955345855156222\n",
            "Average Precision: 0.799912665279091\n",
            "Average Recall: 0.39837905236907734\n",
            "Average F1: 0.5213841599405524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcp8NPIIasEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}